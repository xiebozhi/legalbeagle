https://blogs.loc.gov/law/2013/03/frequent-reference-question-how-many-federal-laws-are-there/

In an example of a failed attempt to tally up the number of laws on a specific subject area, in 1982 the Justice Department tried to determine the total number of criminal laws. In a project that lasted two years, the Department compiled a list of approximately 3,000 criminal offenses. This effort, headed by Ronald Gainer, a Justice Department official, is considered the most exhaustive attempt to count the number of federal criminal laws. In a Wall Street Journal article about this project, “this effort came as part of a long and ultimately failed campaign to persuade Congress to revise the criminal code, which by the 1980s was scattered among 50 titles and 23,000 pages of federal law.”

#Bobb's note: the above makes me angry, now that I'm interested in webcrawling relevant federal laws :/

https://uscode.house.gov/browse/prelim@title34/subtitle1/chapter101/subchapter1&edition=prelim 
https://www.govinfo.gov/app/collection/comps/1
https://www.govinfo.gov/app/collection/uscode 

https://www.supremecourt.gov/opinions/USReports.aspx
https://www.supremecourt.gov/opinions/slipopinion/23



https://docs.scrapy.org/en/latest/topics/media-pipeline.html
https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3
https://webscraping.fyi/lib/python/autoscraper/ 



https://zhongtr0n.medium.com/build-a-web-scraping-api-in-under-30-minutes-with-chatgpt-and-azure-functions-9b48f98b3fae


https://zaytrics.com/how-to-use-llm-to-create-an-expert-system/
Excerpt: 
Steps to Create an Expert System with LLM
1. Define the Problem Domain: The first step in creating an expert system with LLM is to define the problem domain. This could be anything from medical diagnosis to financial planning. The more specific your problem domain, the easier it will be to collect relevant data.

2. Gather Data: The next step is to gather data and information about the problem domain. This could include research papers, articles, and expert opinions. The more data you have, the better your expert system will be.

3. Create a Knowledge Graph: Once you have collected your data, the next step is to create a knowledge graph. You can use tools like Neo4j or Stardog to create your knowledge graph. Nodes in your knowledge graph could represent entities such as symptoms, diseases, or treatments, while edges could represent relationships between them.

4. Train the Language Model: After you have created your knowledge graph, the next step is to train the language model. You can use libraries like Hugging Face’s Transformers or Google’s TensorFlow to train your model. The goal of training your model is to teach it how to generate text based on the knowledge graph.

5. Test and Refine: Once you have trained your model, the next step is to test it with real-world data and refine it based on feedback. This will help you improve the accuracy and effectiveness of your expert system over time.

Choosing the Right Language Model for Your Expert System

When creating an expert system with LLM, it is crucial to select the appropriate language model for the task at hand. While LLM is a powerful tool for generating text based on a knowledge graph, there are several other language models that may be better suited for certain tasks. Here is a few popular language models to consider:

1. GPT-3: GPT-3 is a powerful language model that has been trained on a massive amount of data. It is known for its ability to generate human-like text, making it a popular choice for various applications, including chatbots, content creation, and language translation. While it may not provide a deep understanding of specific domains, it can still be useful in generating general information and responses for an expert system. This can be particularly helpful when dealing with complex questions or when there is a need for a more conversational tone.

2. BERT: BERT is a highly effective language model that excels in natural language understanding and sentiment analysis. It has been trained on large amounts of text data and can be fine-tuned for specific domains, allowing it to provide high-quality text generation that aligns with the context and requirements of the expert system. This makes it an ideal choice for expert systems that require a deep understanding of a particular domain, such as medical diagnosis or legal advice.

3. RoBERTa: RoBERTa builds upon BERT’s architecture and has been trained on even larger amounts of data, resulting in improved performance in various natural language processing tasks. By fine-tuning RoBERTa for specific domains, it can generate accurate and contextually aware text, enhancing the effectiveness of the expert system. This makes it an ideal choice for expert systems that require a high degree of accuracy and contextually relevant text generation.

4. ALBERT: ALBERT is a lightweight version of BERT that achieves comparable performance with fewer parameters. It is designed to be more memory-efficient, making it a suitable choice for resource-constrained environments. By fine-tuning ALBERT, you can create expert systems that provide efficient and accurate text generation. This makes it an ideal choice for expert systems that require high performance in memory-limited environments.

5. T5: T5 is a versatile language model that can be fine-tuned for a wide range of tasks, including text generation. It has achieved state-of-the-art results in various natural language processing benchmarks, making it a powerful tool for creating expert systems. By leveraging T5, you can create expert systems that offer high-quality and contextually relevant text generation, making it an ideal choice for applications where accuracy and relevance are critical.

6. LangChain: LangChain is a language model designed to generate text in multiple languages. It can be fine-tuned for specific domains and can provide high-quality text generation in various languages, making it an ideal choice for expert systems that require multilingual support.

7. GPTforALL: GPTforALL is a language model designed to be more inclusive and accessible. It has been trained on diverse datasets and can provide text generation that is more representative of different cultures and backgrounds. This makes it an ideal choice for expert systems that require more inclusive and diverse text generation.

8. Private GPT: Private GPT is a language model designed to provide secure and private text generation. It can be used in applications where data privacy is critical, such as healthcare or finance. Private GPT can be fine-tuned for specific domains while ensuring the privacy and security of sensitive information.



Bobb's note: Thinking from the above that RoBERTa is the right model, seeking a howto on how to accomplish it:

https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c

Legal Dictionary: https://www.nolo.com/dictionary/a

